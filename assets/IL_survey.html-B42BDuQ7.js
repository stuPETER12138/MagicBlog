import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,b as t,o as a}from"./app-u0NPXd-N.js";const o="/assets/bc.drawio-DzwzkJCn.png",r={};function s(l,e){return a(),n("div",null,e[0]||(e[0]=[t('<h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><p>In recent years, artificial intelligence (AI) technologies represented by deep learning (DL) have shined in robotic tasks, and the concept of embodied intelligence has emerged. A typical representative technology among them is Imitation Learning (IL). We know that robots are a bridge between virtual programs and the real world, but in most cases, robots cannot get feedback through the real world, which is the so-called reward. Therefore, the setting of the reward function is crucial. Traditional methods require experts to provide specific, hard-coded rules regarding the actions that a machine must perform, as well as the characteristics of the environment in which the machine operates. However, developing such rules requires considerable time and coding expertise . Therefore, researchers hope that robots can learn human behavior and solve problems in the same way as humans. This is the basic idea of Imitation Learning.</p><p>As robotic tasks continue to emerge, researchers have designed imitation learning algorithms with their own characteristics. In this survey, we provide a comprehensive review of imitation learning, aiming to help novices understand its working principles and provide experienced practitioners with the latest developments. Specifically, the survey is organized as follows: In Section 2, we will review the basic concepts of imitation learning in detail, introduce its mathematical formulas, development history, and main advantages. Then, in Section 3, we will explore the performance and effects of imitation learning in practical applications, and focus on its innovative applications in the field of embodied intelligence. Then, in Section 4, we will explain the challenges and limitations of imitation learning. Finally, in Section 5, we will summarize the contents of the entire survey.</p><h2 id="imitation-learning" tabindex="-1"><a class="header-anchor" href="#imitation-learning"><span>Imitation Learning</span></a></h2><p>The main purpose of Imitation Learning is to enable agents to learn to perform a specific task or behavior by imitating an expert through the provision of demonstrations.It can be divided into two categories: behavior cloning (BC) and inverse reinforcement learning (IRL). Furthermore, we will explore Adversarial Imitation Learning (AIL) algorithms that extend IRL by introducing adversarial environments. We will highlight the benefits of integrating adversarial training into IL and evaluate the current progress in the field of AIL.</p><h3 id="behavior-cloning" tabindex="-1"><a class="header-anchor" href="#behavior-cloning"><span>Behavior Cloning</span></a></h3><figure><img src="'+o+'" alt="Figure: A Flow Chart of Behavior Cloning where s is the input and a is the output" tabindex="0" loading="lazy"><figcaption>Figure: A Flow Chart of Behavior Cloning where s is the input and a is the output</figcaption></figure><p>BC is an IL technique that treats the problem of learning a behavior as a supervised learning task. Given the expert state-action pairs, as shown in Figure, the embodied intelligent robot can predict the actions it should take in a particular state by training a classifier or regression model. It can be seen that Behavior cloning is the direct learning strategy through expert demonstration, that is, learning a mapping from state to action.</p><p>Obviously, BC has a serious problem, that is, it is very dependent on the data set. Once the robot encounters a scene outside the data set, it will behave uncontrollably, thus losing its reference significance, which is very inappropriate in real applications.</p><h3 id="inverse-reinforcement-learning" tabindex="-1"><a class="header-anchor" href="#inverse-reinforcement-learning"><span>Inverse Reinforcement Learning</span></a></h3><p>In addition to behavioral cloning, another key approach to imitation learning is Inverse Reinforcement Learning (IRL). The goal of IRL is to infer the reward function from the expert&#39;s behavior, that is, the agent learns a reward model so that the expert&#39;s behavior is considered optimal under this model. Unlike behavioral cloning, IRL does not directly imitate the expert&#39;s actions, but learns the reward function to understand why the expert takes these actions. Once the robot has mastered the reward function, it can use traditional reinforcement learning methods to optimize its own strategy and make better decisions in the same or similar environments.</p><p>IRL has been widely used in a variety of applications, such as robotics manipulation, autonomous navigation, game playing, and natural language processing. However, designing an effective and usable IRL is very challenging. There are two important reasons for this.</p><ul><li><p>IRL can be computationally expensive and resource-intensive. This is partly because the robot needs to continuously learn the correct reward function from the feedback of its interactions with the real world.</p></li><li><p>A typical IRL approach follows an iterative process that involves alternating between reward estimation and policy training, which results in poor sample efficiency.</p></li></ul><h3 id="adversarial-imitation-learning" tabindex="-1"><a class="header-anchor" href="#adversarial-imitation-learning"><span>Adversarial Imitation Learning</span></a></h3><p>The advantages of AIL can well solve some of the limitations of IRL. The first AIL method that gained prominence is known as generative AIL (GAIL) \\cite{ho2016generativeadversarialimitationlearning}. GAIL combines generative adversarial networks (GANs) with imitation learning, using a generative adversarial mechanism to approximate the behavior of experts. The generator is responsible for generating actions similar to those of experts, while the discriminator tries to distinguish whether these actions come from experts. Through this adversarial process, the agent can learn behavioral strategies similar to those of experts. Over the years, numerous improvements have been proposed to the original algorithm to improve its sample efficiency, scalability, and robustness, including changes to the discriminator’s loss function and switching from on-policy to off-policy agents</p><h2 id="applications-and-impact" tabindex="-1"><a class="header-anchor" href="#applications-and-impact"><span>Applications and Impact</span></a></h2><h3 id="algorithm-development-and-application" tabindex="-1"><a class="header-anchor" href="#algorithm-development-and-application"><span>Algorithm Development and Application</span></a></h3><p>With the raise of Transformers in NLP and CV domains, modern imitation learning methods have adopted Transformers as their backbone. In addition, the application of diffusion model provides advantages for imitation learning. Due to their strong generalization ability and rich representation power to capture multimodal action distributions, they have become the SoTA in the imitation learning domain. Not only that, researchers also tried to combine the mamba model with imitation learning. The introduction of Mamba in an encoder-decoder structure enhances its versatility, making it suitable for standalone use as well as integration into advanced architectures like diffusion processes.</p><h3 id="combining-simulation-with-reality" tabindex="-1"><a class="header-anchor" href="#combining-simulation-with-reality"><span>Combining Simulation With Reality</span></a></h3><p>Recently, several sim-to-real paradigms have been introduced, to mitigate the need for extensive and costly real-world demonstration data by conducting extensive learning in simulation environments, followed by migration to real-world settings.</p><ul><li><p>Real2Sim2real: It enhanced Imitation learning in real-world scenarios by leveraging reinforcement learning RL trained in a “digital twin” simulation environment</p></li><li><p>TRANSIC: It enhances sim-to-real transfer performance through several steps: first, robots are trained using RL to establish foundational strategies within a simulation environment. Then, these strategies are implemented on real robots, with humans intervening and correcting behaviors in real-time via remote control when errors occur.</p></li></ul><h3 id="robot-learning-and-control" tabindex="-1"><a class="header-anchor" href="#robot-learning-and-control"><span>Robot Learning and Control</span></a></h3><p>Imitation learning aims to minimize data usage by collecting high-quality demonstrations. To improve data efficiency, reduce interaction costs, and ensure safety, Offline RL + Online RL is proposed. In this approach, offline RL is first used to learn policies from a static large-scale dataset collected in advance. These strategies are then deployed in real environments for real-time interaction and exploration, and adjustments are made based on feedback. The representative Imitation Learning methods from human demonstrations are ALOHA.</p><h2 id="challenges-and-limitations" tabindex="-1"><a class="header-anchor" href="#challenges-and-limitations"><span>Challenges and Limitations</span></a></h2><p>A common assumption in IL methods is that the demonstrations will be optimal, performed by an expert demonstrator. However, this assumption is too restrictive when it comes to learning from demonstrations in a variety of cases. First, it is very difficult to obtain a large number of high-quality human expert demonstrations, as this requires a lot of time and effort. In addition, humans are easily affected by various factors such as link interference and physical fatigue when performing tasks, which makes the demonstration data lose its reference value to a certain extent. Second, combining demonstration data obtained from different human experts can improve the diversity of the dataset and the stability of the model. However, a crowdsources dataset will inevitably have a wide range of behavior optimality since it is collected from users with varying levels of expertise.</p><p>The naive solution to imperfect demonstrations would be to discard the non-optimal ones. However, this screening process is often impractical since it requires significant human effort. Therefore, researchers have been increasingly interested in developing methods that can learn from imperfect demonstrations. By effectively leveraging human demonstration data, robotic systems can achieve higher levels of performance and adaptability, enabling them to more effectively perform complex tasks in dynamic environments.</p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion"><span>Conclusion</span></a></h2><p>Imitation learning has demonstrated significant value in embodied intelligence by enabling robots to efficiently acquire complex behaviors through expert demonstrations. This approach mitigates the necessity for extensive trial-and-error processes and facilitates quicker adaptation to real-world tasks. Nevertheless, challenges persist, including the dependence on high-quality demonstrations and the requirement for improved generalization to novel scenarios. Future advancements in algorithms and data integration are expected to further enhance the capabilities of imitation learning, paving the way for more sophisticated and autonomous robotic systems.</p>',28)]))}const d=i(r,[["render",s],["__file","IL_survey.html.vue"]]),m=JSON.parse('{"path":"/thinking/college/IL_survey.html","title":"A Survey of Imitation Learning for Embodied Intelligence","lang":"zh-CN","frontmatter":{"title":"A Survey of Imitation Learning for Embodied Intelligence","date":"2025-01-09T00:00:00.000Z","category":["我思"],"tag":["IL","survey"]},"headers":[{"level":2,"title":"Introduction","slug":"introduction","link":"#introduction","children":[]},{"level":2,"title":"Imitation Learning","slug":"imitation-learning","link":"#imitation-learning","children":[{"level":3,"title":"Behavior Cloning","slug":"behavior-cloning","link":"#behavior-cloning","children":[]},{"level":3,"title":"Inverse Reinforcement Learning","slug":"inverse-reinforcement-learning","link":"#inverse-reinforcement-learning","children":[]},{"level":3,"title":"Adversarial Imitation Learning","slug":"adversarial-imitation-learning","link":"#adversarial-imitation-learning","children":[]}]},{"level":2,"title":"Applications and Impact","slug":"applications-and-impact","link":"#applications-and-impact","children":[{"level":3,"title":"Algorithm Development and Application","slug":"algorithm-development-and-application","link":"#algorithm-development-and-application","children":[]},{"level":3,"title":"Combining Simulation With Reality","slug":"combining-simulation-with-reality","link":"#combining-simulation-with-reality","children":[]},{"level":3,"title":"Robot Learning and Control","slug":"robot-learning-and-control","link":"#robot-learning-and-control","children":[]}]},{"level":2,"title":"Challenges and Limitations","slug":"challenges-and-limitations","link":"#challenges-and-limitations","children":[]},{"level":2,"title":"Conclusion","slug":"conclusion","link":"#conclusion","children":[]}],"git":{"createdTime":1736435601000,"updatedTime":1736435601000,"contributors":[{"name":"Wu","email":"jio12138@qq.com","commits":1}]},"readingTime":{"minutes":4.79,"words":1438},"filePathRelative":"thinking/college/IL_survey.md","localizedDate":"2025年1月9日","excerpt":""}');export{d as comp,m as data};
